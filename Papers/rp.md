1.
My research primarily focuses on designing generalizable 3D reconstruction (G3R) framework based on multi-view geometry. Specifically, we aim to leverage geometric information from multiple views to train a model capable of reconstructing novel objects' geometry beyond the training dataset.Furthermore, the proposed model must demonstrate rapid reconstruction capability, low computational overhead, and high geometric quanlity. For example, generalizable reconstruction capabilities allow autonomous driving perception systems to adapt to previously unseen environments. Additionally, it enables the reconstruction of diverse models for animation and AAA games, significantly reducing the workload of production teams.
Recently, the emergence of methods such as MVS (Multi-View Stereo), Neural Radiance Field (NeRF), and 3D Gaussian Splatting (3DGS) has opened up new perspectives for 3D research. MVSNet leverages feature matching across multi-views, enabling the network to learn geometric correspondences, thereby demonstrating strong generalization capabilities. NeRF and 3D GS employ implicit and explicit geometric representations respectively,both achieving photorealistic novel view synthesis (NVS) through differentiable volume rendering. Moreover, 3D GS accelerates the rendering process through CUDA-based parallelism, significantly improving training efficiency and establishing itself as a prominent direction in contemporary 3D vision research. Building upon these advancements, Our baseline is mainly based on 3D GS, enhanced by integrating geometric matching from MVS, aiming to achieve fast and efficient 3D reconstruction. 
Current G3R methods first obtain multi-view geometry (depth and feature maps) via MVS, then integrate them through implicit networks for 3DGS processing—an approach that imposes heavy training burdens and resource barriers. Therefore, rather than generalizable research, our foremost priority should be enhancing the completeness and accuracy of 3D GS for surface reconstruction. This would enable acquiring geometric structures with lesser computational time and resources. Current surface reconstruction predominantly rely on mesh-based approaches. The core issue is that 3DGS was initially designed for NVS, with no consideration for spatial geometric constraints. Although 3DGS appears to be an explicit geometric method, this explicitness refers to its use of point cloud data as input—a discrete data representation. As a result, gaussians do not consistently conform to underlying surface constraints during optimization. By observing the reproduction process, these biases were found to be particularly pronounced in regions with transparency, specular highlights, and shadows. Therefore, even though 3DGS achieves high-quality 2D rendering, significant artifacts—such as holes and loss of detail—often appear when converting the point cloud into a mesh. Subsequent attempts to constrain 3D Gaussians using 2D planar priors partially address this, but the constraints themselves rely on semi-explicit functions. This approach fails to provide adequate spatial-geometric guidance or optimization, leaving the underlying geometric fidelity unresolved.

Accordingly, the current research plan is divided into three main components:
(1) Addressing the spatial discretization issue of point clouds in 3DGS to ensure that the reconstructed geometry remains tightly aligned with the object surface;
(2) Tackling the problem of single-scene reconstruction to improve overall reconstruction efficiency; and
(3) Mitigating the limitations of G3R, which currently requires substantial computational resources and hinders broader research participation, by accelerating training and reducing resource consumption.
