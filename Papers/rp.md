1.
My research primarily focuses on designing generalizable 3D reconstruction (G3R) framework based on multi-view geometry. Specifically, we aim to leverage geometric information from multiple views to train a model capable of reconstructing novel objects' geometry beyond the training dataset.Furthermore, the proposed model must demonstrate rapid reconstruction capability, low computational overhead, and high geometric quanlity. For example, generalizable reconstruction capabilities allow autonomous driving perception systems to adapt to previously unseen environments. Additionally, it enables the reconstruction of diverse models for animation and AAA games, significantly reducing the workload of production teams.

Recently, the emergence of methods such as MVS (Multi-View Stereo), Neural Radiance Field (NeRF), and 3D Gaussian Splatting (3DGS) has opened up new perspectives for 3D research. MVSNet leverages feature matching across multi-views, enabling the network to learn geometric correspondences, thereby demonstrating strong generalization capabilities. NeRF and 3D GS employ implicit and explicit geometric representations respectively,both achieving photorealistic novel view synthesis (NVS) through differentiable volume rendering. Moreover, 3D GS accelerates the rendering process through CUDA-based parallelism, significantly improving training efficiency and establishing itself as a prominent direction in contemporary 3D vision research. Building upon these advancements, Our baseline is mainly based on 3D GS, enhanced by integrating geometric matching from MVS, aiming to achieve fast and efficient 3D reconstruction. 

Current G3R methods first obtain multi-view geometry (depth and feature maps) via MVS, then integrate them through implicit networks for 3DGS processing—an approach that imposes heavy training burdens and resource barriers. Therefore, rather than generalizable research, our foremost priority should be enhancing the completeness and accuracy of 3D GS for surface reconstruction. This would enable acquiring geometric structures with lesser computational time and resources. Current surface reconstruction predominantly rely on mesh-based approaches. The core issue is that 3DGS was initially designed for NVS, with no consideration for spatial geometric constraints. Although 3DGS appears to be an explicit geometric method, this explicitness refers to its use of point cloud data as input—a discrete data representation. As a result, gaussians do not consistently conform to underlying surface constraints during optimization. By observing the reproduction process, these biases were found to be particularly pronounced in regions with transparency, specular highlights, and shadows. Therefore, even though 3DGS achieves high-quality 2D rendering, significant artifacts—such as holes and loss of detail—often appear when converting the point cloud into a mesh. Subsequent attempts to constrain 3D Gaussians using 2D planar priors partially address this, but the constraints themselves rely on semi-explicit functions. This approach fails to provide adequate spatial-geometric guidance or optimization, leaving the underlying geometric fidelity unresolved.

Accordingly, the current research plan is divided into three main components:
(1) Addressing the spatial discretization issue of point clouds in 3DGS to ensure that the reconstructed geometry remains tightly aligned with the object surface;
(2) Tackling the problem of single-scene reconstruction to improve overall reconstruction efficiency; and
(3) Mitigating the limitations of G3R, which currently requires substantial computational resources and hinders broader research participation, by accelerating training and reducing resource consumption.

2. 
In this section, I will analyze the issues identified previously and propose several targeted solutions to be implemented. Prior to conducting the experiments, it is necessary to prepare the required computational resources. At present, I have access to a 24GB NVIDIA RTX 4090, which I am using for improving the completeness and accuracy of geometric reconstruction. As I need to continuously follow and implement the latest advancements and methodologies in this research area, I plan to purchase an additional RTX 4090 GPU to ensure a total of at least 40GB of VRAM for training genrelizable models.

This research plan focuses on three issues currently faced in mesh-based 3DGS reconstruction. The first problem addresses the presence of surface holes and loss of fine details in 3DGS during the reconstruction process.
To help readers better understand the proposed improvements, we begin with a brief overview of the 3DGS pipeline. The input to 3DGS is a preprocessed point cloud, where each point is associated with color information (used to compute spherical harmonics), spatial coordinates, and camera parameters from a specific viewpoint. During initialization, these camera parameters are used to compute a covariance matrix for each point, which determines the initial scale of the corresponding 3D Gaussian primitives. 3DGS first constructs the 3D Gaussian primitives and their attributes in world coordinates. These are then transformed through a series of coordinate conversions into camera and image space, resulting in 2D Gaussian projections with associated attributes (such as opacity, scale, and color). Using volume rendering techniques, 3DGS synthesizes a rendered image from the perspective of the current camera view. This rendered image is compared with the GT image, and the discrepancy is used to update the Gaussian parameters through backpropagation. Simultaneously, Gaussian primitives are selectively duplicated or removed based on their contribution to the reconstruction quality. After optimizing one viewpoint, 3DGS proceeds to the next, repeating this process iteratively to refine the global 3D structure.

From the overall network structure, it is evident that no explicit geometric constraints are imposed on the 3DGS. As a result, the 3D Gaussian primitives (point clouds) are not necessarily adhered to the actual geometric surfaces. Current methods attempt to address this by introducing geometric constraints in the form of depth maps or planes. However, it is important to note that 3D Gaussian primitives are optimized within a semi-explicit function space, which inherently introduces certain inconsistencies in the data itself. Moreover, the depth maps or planes currently employed are derived from the attributes of 2D Gaussians, and although they do introduce geometric supervision into the network, they remain susceptible to spatial perturbations along the surface due to the limitations of 3D Gaussian primitives.  That said, depth maps still serve as a valuable explicit geometric representation. One of their main advantages lies in their 2D nature, which ensures relatively low computational and memory overhead.

Inspired by CasMVSNet, our proposed method introduces a depth range into the input data and similarly defines 48 depth planes (tentatively, subject to change in future experiments). Unlike existing approaches, our depth map is generated at the initialization stage. Since the camera parameters for each point cloud are known, we can compute the distance from each point to its corresponding camera, i.e., the ground-truth depth. However, as the number of 3D Gaussian primitives increases progressively during training, directly using the depth of the point cloud to generate depth maps becomes infeasible. To address this, we first calculate the average depth of the current set of point clouds to define a central depth plane. Subsequently, for regions above and below this central plane, we compute additional average depth values within those respective regions, thereby constructing a hierarchical depth range composed of multiple depth planes. During the rendering process, 3DGS employs volumetric rendering function, which accumulates the contributions of 2D Gaussian primitives weighted by their opacities. Drawing on this idea, we approximate the opacity as a probability, selecting the nearest 2D Gaussian primitive to each initialized depth plane and using this as the plane’s representative probability value. A similar volumetric rendering strategy is then applied to obtain a depth map from the current viewpoint. During the rendering process, 3DGS employs volumetric rendering function, which accumulates the contributions of 2D Gaussian primitives weighted by their opacities. Drawing on this idea, we approximate the opacity as a probability, selecting the nearest 2D Gaussian primitive to each initialized depth plane and using this as the plane’s representative probability value. A similar volumetric rendering strategy is then applied to obtain a depth map from the current viewpoint. Unlike standard 3DGS pipelines, where the depth estimation evolves with the increasing number of primitives, our method estimates depth for the entire target region at the outset, avoiding additional computational burden during training. Moreover, like recent methods, we apply a multi-view geometric consistency regularization. However, instead of estimating point cloud poses, we directly enforce consistency on the depth values of the 3D Gaussian primitive centers across multiple views. This allows us to effectively constrain the depth planes, which in turn constrains the spatial arrangement of the 3D Gaussian primitives. Through this geometric regularization, the 3D Gaussian primitives are encouraged to align more accurately with the actual object surfaces, thereby improving both the completeness and accuracy of the reconstruction. Consequently, this approach effectively mitigates the common issue of holes in reconstructed geometry.
Additionally, to address the issue of detail loss in the reconstruction, we can derive normal vectors from the depth map. These normal vectors provide the surface's curvature, which offers a key advantage: they can effectively indicate the complexity of the surface. By weighting the complexity, the network can be guided to focus more on detailed areas, thus alleviating the issue of missing details in the reconstruction. Regarding the geometric challenges of 3D Gaussian primitives, I aim to resolve these issues within eight months (4 months + 4 months), ensuring that the network achieves SOTA performance. Subsequently, we will examine the trade-offs associated with these results. If we can successfully guide the 3D Gaussian primitives to align with the geometric surfaces, this will lead to improvements in both the time efficiency and memory usage of the network.

The work on NVS for 3DGS concludes at this stage. After obtaining a sufficiently complete and accurate point cloud, the next step is to convert the point cloud into an ideal mesh for surface reconstruction. Ideal surface reconstruction requires ensuring a smooth and flat surface, a complete geometric structure, and clear details at the boundaries. Current methods mainly rely on truncated signed distance fields (TSDF) and Poisson surface reconstruction, both of which are explicit surface reconstruction techniques. This means that surface reconstruction is performed on a scene-by-scene basis, which incurs significant time overhead and fails to achieve an ideal surface reconstruction result.
To advance our generilizable research, we propose utilizing implicit surface functions for this part. A lower-dimensional plane can be considered as the zero-level set of a higher-dimensional function, and thus designing high-dimensional implicit surface functions can better conform to lower-dimensional geometric surfaces. The advantages of designing such implicit surface functions are as follows: First, this part can be trained together with the previous NVS network, allowing us to perform end-to-end training. Moreover, implicit surface functions exhibit strong generalization capability for surface reconstruction of similar objects, thus reducing time overhead. Finally, implicit surface functions offer better reconstruction of finer details.
Similarly, we aim to address this for the object within three months. After finalizing the network structure and achieving SOTA results through debugging, we will then examine the trade-offs associated with these results in order to optimize the network's time and memory efficiency.

3.
Currently, most existing G3R methods tend to adopt brute-force strategies—simple additive designs like A+B network architectures might yield promising results but often introduce significant noise and instability. Assuming the first two proposed solutions successfully capture complete and accurate surface geometry—while also accounting for time and memory efficiency at each stage—we proceed by leveraging the depth intervals defined in Solution 1 as the foundation for our generalizable network design. Specifically, we adopt the CasMVSNet architecture as our backbone network.

Solution 1 ensures that 3DGS training does not result in an excessive number of Gaussian primitives. Based on the initialized point cloud, we construct a 3D spatial coordinate system (xyz) and directly map each point to a voxel grid, where each voxel represents a potential 3D Gaussian primitive center and its corresponding features. Inspired by 2DGS, we discard the traditional 3D Gaussian representation and instead adopt 2D Gaussians as our geometric representation within 3D space.


CasMVSNet takes multi-view feature maps as input. For each 2D Gaussian primitive, we map it to a voxel coordinate and assign a feature vector. Through stereo matching, we construct a cost volume across multiple views and regularize it to obtain the probability distribution over voxel locations corresponding to each 2D Gaussian. We then apply volume rendering to synthesize images and support subsequent implicit surface reconstruction tasks. By optimizing the rendered images, we iteratively refine the positions and attributes of the 2D Gaussian primitives in 3D space.

Since the 2D Gaussians, as established in Solution 1, are tightly aligned with actual surfaces, this approach avoids the heavy computational burden often associated with stereo matching of traditional 3D Gaussians. As a result, we achieve a fast, efficient, and generalizable reconstruction network.

因此，从整个网络结构上可以看出，整个网络并没有对3D高斯基元进行几何约束，因此3D高斯的点（点云）在空间中并不一定拟合真实几何表面。目前的研究方法采用了深度图或者深度平面这样的几何来约束。但是有一点需要注意的是，由于3D高斯基元是在半显式函数进行优化和训练，因此3D高斯基元的数据本身就存在一定的问题，而目前的方法采用的深度图或者深度平面都是基于2D高斯的属性来获取的，虽然他给予了网络一定的几何约束，但是他依然受限于3D高斯基元在几何表面的上下扰动。但是，深度图是一个很好的显式几何表示，他的优势在于他是一个2D的数据，因此他消耗的资源不多。
我的方案受CasMVSNet的启发，我们在输入数据中添加了一个深度范围，并同样设置48个深度平面（假设，后续实验会改动）。与目前的方法不同，我们的深度图在初始化的时候就产生了。同时，由于我们可以获得每个点云的相机参数，因此可以获得每个点云与相机的距离，也就是每个点云的深度真值，由于3D高斯基元会随着网络的推进而逐渐增多，直接拿点云的深度图并不可取。因此，我们首先获取这些点云的平均深度值作为深度平面的中心，然后对于中心的上下部分也依次采用同区域内的平均深度值作为区域的中心深度平面，最终构成了一个多深度平面的深度区间范围。后续进行渲染的时候，后续3dgs在渲染过程中3dgs会运用到体渲染函数，他是通过对每个2D高斯基元乘以所含有的不透明度来叠加获取最终的渲染图。同理，我们把这个不透明度近似为概率度，我们根据初始化的深度平面选择最接近的3D高斯基元作为该深度平面的概率值，并运用类似的体渲染函数计算来获取该视角下的深度图。与3D高斯基元不同的是，我们在初始阶段就对于目标的所有区域进行的深度估计，因此我们的深度图计算不会随着3D高斯基元数据的增多而带来更多的计算负担。此外，我们与目前方法一样，引用一个多视图几何一致性的正则化，与他们不同的是，他们是计算点云的位姿，而我们是直接计算不同视角下3D高斯基元中心点所在的深度一致性，这样可以很好的去约束深度平面，从而去约束3D高斯基元的空间几何位置。通过这样的几何约束，我们可以让3D高斯基元贴合在更加理想化的物体表面上，从而提高网络重建的完整性和准确性，因此这种方法可以很好的提高重建的孔洞问题。
此外，对于重建的细节缺失问题，我们可以通过深度图得到对应的法线向量，法线向量会带来每个平面的法曲率。法曲率的一个优势在于，他可以很好的判断该表面是否复杂，从而通过复杂性的权重来让网络更专注于细节区域，这样就可以很好的解决重建的细节缺失问题。
对于3D高斯基元的几何问题，我希望能在8个月之后得到解决（4月+4月），保证网络达到sota效果。后续我们再去考虑这些结果的损耗问题，如果可以很好的引导3D高斯基元在几何表面，那么3D从而改善我们网络的时间和内存损耗。
2. 对于3DGS的NVS工作到上述就已经结束了，但对于表面重建任务，这以为对于表面重建的3DGS，他把整体人物分为了两个部分进行。在获得了足够完整和准确的点云之后，我们需要改进的就是如何把点云转变为理想的mesh，从而进行表面重建。理想的曲面重建需要保证表面平整光滑，完整的几何结构，且交界处细节分明。目前的工作主要利用截断符号距离场和柏松表面重建来进行，这些方式都是显式的曲面重建，这就意味着他是逐场景进行表面重建的，这带来了巨大的时间损耗，此外，他并不能达到理想化的曲面重建效果。为了能更好的推进我们的可泛化性研究，并重建更多细节化的区域，我们将这部分引用隐式曲面函数来解决，低维的平面就是高维的零水平集，因此设计高维隐式曲面函数可以很好的贴合低维的几何表面。设计这样的隐式曲面函数有这样几个优点：一方面，我们可以把这部分和上一部分放在一起训练，这样的好处是，我们可以端到端的训练网络，此外，隐式曲面函数对同一物种的的曲面重建具有良好的泛化能力，从而可以减少一定的时间损耗。最后，隐式曲面函数可以更好的重建细节部分。
同样的，这个人物也希望能在3个月内得到解决。同样，我们在完成网络结构后，并调试以达到sota的结果，我们再去考虑这些结果的损耗问题，从而改善我们网络的时间和内存损耗。
3.目前对于G3R的方法都过于暴力，简单的A+B网络虽然可以达到效果，但也带来了很多其他的噪声干扰。假设前两种方案可以很好的获得完整和准确的几何表面，并且每个过程后也同时考虑了时间和内存损耗的影响。因此我们以方案1所设定深度区间作为深度值，并采用CasMVSNet的网络结构作为我们可泛化网络的基准。方案1可以保证我们在3DGS推进中不会造成过多3D高斯基元，因此，我们可以以初始化的点云在基准，构建一个（xyz）的三维坐标，并直接将每个点云映射到三维坐标系的体素中，每个体素表示一个3D高斯基元中心的特征。受到2DGS的启发，我们舍弃了3D高斯基元的选择，而是直接采用2D高斯基元作为三维空间中的高斯几何表示。CasMVSNet以多个视图的特征图为输入，我们将每个2D高斯基元映射一个体素坐标及所表示的特征值，通过立体匹配，我们可以得到不同视角下的特征的匹配代价体，对这个代价体进行正则化，我们可以每个2D高斯基元映射的体素位置的概率值，然后再采用体渲染的方法来获取渲染图片，以及后续的隐式表面重建任务。然后通过优化这个渲染图片，我们可以优化三维空间中的2D高斯基元，由于这个2d高斯基元如方案1所示可以很好的贴合几何表面，因此我们不用考虑2d高斯基元给立体匹配带来的计算负担，从而达到快速，高效的的可泛化网络的效率。
